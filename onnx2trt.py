import os
import tensorrt as trt
import onnx
#nvidia_tensorrt-8.4.3.1
def convert_onnx_to_trt(onnx_path: str,
                        trt_path: str,
                        max_workspace_size: int = 1 << 30,
                        fp16: bool = False,
                        input_shape=(1, 3, 400, 800)):
    """
    å°† ONNX æ¨¡åž‹è½¬ä¸º TensorRT å¼•æ“Žå¹¶ä¿å­˜åˆ°æ–‡ä»¶ï¼Œå¸¦åŠ¨æ€è¾“å…¥ä¼˜åŒ–é…ç½®
    :param onnx_path: ONNX æ¨¡åž‹è·¯å¾„
    :param trt_path: ä¿å­˜ TRT å¼•æ“Žè·¯å¾„
    :param max_workspace_size: æœ€å¤§ä¸´æ—¶æ˜¾å­˜ï¼ˆå­—èŠ‚ï¼‰
    :param fp16: æ˜¯å¦å¯ç”¨ FP16 åŠ é€Ÿ
    :param input_shape: ONNX ä¸­ 'input' å¼ é‡çš„ (min/opt/max) shape ç¤ºä¾‹
    """
    assert os.path.exists(onnx_path), f"ONNX æ–‡ä»¶ä¸å­˜åœ¨: {onnx_path}"
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

    # 1. åˆ›å»º Builder/Network/Parser
    builder = trt.Builder(TRT_LOGGER)
    flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(flags)
    parser = trt.OnnxParser(network, TRT_LOGGER)

    # 2. è§£æž ONNX
    with open(onnx_path, 'rb') as f:
        if not parser.parse(f.read()):
            for i in range(parser.num_errors):
                print(parser.get_error(i))
            raise RuntimeError("âŒ ONNX è§£æžå¤±è´¥")

    # 3. åˆ›å»º BuilderConfig å¹¶è®¾ç½®å·¥ä½œåŒº
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)
    if fp16:
        config.set_flag(trt.BuilderFlag.FP16)

    # 4. **åˆ›å»ºå¹¶æ·»åŠ  Optimization Profile**  
    profile = builder.create_optimization_profile()
    # å‡è®¾ä½ çš„ç½‘ç»œç¬¬ä¸€ä¸ªè¾“å…¥åå« "input"
    # ä½ éœ€è¦æ ¹æ®å®žé™…å¯¼å‡ºçš„ ONNX model.input_names æ¥å¡«å†™
    min_shape = input_shape    # e.g. (1,1,240,320)
    opt_shape = input_shape    # e.g. (1,1,240,320) â€”â€” æ—¥å¸¸æŽ¨ç†æ—¶å¸¸ç”¨å°ºå¯¸
    max_shape = (1, 3, 400, 800)  # å¯æ”¯æŒçš„æœ€å¤§å°ºå¯¸ï¼Œæ¯”å¦‚ 2x 240Ã—320
    profile.set_shape("input", min_shape, opt_shape, max_shape)
    config.add_optimization_profile(profile)

    # 5. æž„å»º TensorRT å¼•æ“Ž
    print(f"ðŸŸ¢ æ­£åœ¨æž„å»º TRT å¼•æ“Ž (FP16={fp16}) â€¦")
    engine = builder.build_engine(network, config)
    if engine is None:
        raise RuntimeError("âŒ TensorRT å¼•æ“Žæž„å»ºå¤±è´¥")

    # 6. åºåˆ—åŒ–å¹¶ä¿å­˜
    with open(trt_path, 'wb') as f:
        f.write(engine.serialize())
    print(f"âœ… å·²ä¿å­˜ TRT å¼•æ“Žåˆ°: {trt_path}")


if __name__ == "__main__":
    onnx_model = "/home/zc/code/tast_1/weights/superpoint.onnx"
    trt_engine  = "/home/zc/code/tast_1/weights/superpoint.trt"
    m = onnx.load(onnx_model)
    try: 
        onnx.checker.check_model(m) 
    except Exception: 
        print("Model incorrect") 
    else: 
        print("Model correct")
    print([ (o.name, o.type.tensor_type.shape) for o in m.graph.output ])
    # æ³¨æ„ï¼šinput_shapeã€max_shape è¦ä¸Žä½ çš„æ¨¡åž‹å¯¼å‡ºæ—¶çš„ dynamic_axes ä¿æŒä¸€è‡´
    convert_onnx_to_trt(
        onnx_model,
        trt_engine,
        fp16=False,
        input_shape=(1, 3, 400, 800)  # min/opt shape
    )
